Hi Mom and Dad, 
Yay! One more full week of summer vacation.......
We have accomplished a lot since the last email. Last email we had just met Alex, Anton, and Olga and were just getting started on our real project. Now about a month later, we have learned sooooo much more about python and made a lot of progress. We ended up choosing the project on diatoms. Our goal is to create a code that will tell us if the diatom is mutated or not (we are told they look almost identical). We will package the code so that you just read in a directory of images and get out a yes or no for all of the images (hopefully it will be pretty accurate). So first, we imported the picture, changed the colormap, and converted it to a numpy array. Then we thresholded it with the Otsu thresholding method. Then we overlayed the skeleton with the original image using an alpha value (transparency). It was pretty hard to figure all of that out by ourselves (Dr. Steed was at the beach that week), but we managed. So our first attempt had one major problem: the skeleton was much too detailed and went around all of the dots. Less than a week in, Alex asked us to present our work at the weekly group meeting, and we freaked out because 1. we barely knew what we were talking about and 2. he told us literally 18 hours before we were supposed to present. After the meeting and realizing how bad the skeletons looked, our primary focus became fixing the thresholding. They suggested dilating or blurring the image before thresholding and trying different types of both global and local thresholding. We took their advice and tried so many different types of thresholding. We mainly tried Otsu and Yen for global and Adaptive for local; however, we had to test several different types of smoothing techniques on each. We tried dilating and eroding the picture before thresholding as well as opening and closing the skeletons (none of which really helped). We found the 'optimal blur' for otsu and compared histogram shapes and statistics (mean, standard deviation) with the best blur. Blurring improved otsu, but it wasn't great for all of them. Then we tried different types of thresholds (gaussian and mean) and blurs (gaussian, median, average) for adaptive/local thresholding, and none worked particularly well. So we decided to step back and identify what the difference was between pictures that had good and bad skeltons. We concluded that if the pathways were the same or a darker color than the dot-filled areas, then the skeleton would be bad. We then decided that we would need to move some of the colors to other values to increase the color contrast. Initially, I thought we could just move a certain percentage around the mean value to white, but then I realized that there could be a few problems with that approach. The path color could not even be in that color range, and we would have to somehow only do it to 'problem images.' Also, if the dot-filled areas and the pathways were in the same color range, we would be making almost the whole image white, which would defeat the purpose of trying to increase color contrast. I then started to look for functions that would moved color values to a different value and functions that simplify the color data. I stumbled upon the PIL Image quantize function when looking up an error in Paulinka's edge detection experimentation code. This function allows you to choose the number of colors in the image. After testing many different numbers, we decided that 3 colors would be the best because the colors will already be kind of far apart but the general appearance of the image will be preserved. We manually moved the maximum color value is moved to 255 (white) and the minimum value is moved to 0 (black) (this is now automated). There will only be one color in the middle, usually somewhere between 85 and 120. This function seemed very promising, so we continued to experiment with it. We first tried Otsu's thresholding after quantizing the image. It didn't work very well because the function just chose one of the 3 values and removed the rest. We tried blurring the image before thresholding and it helped, but we decided to keep trying other options. Then we tried using adaptive thresholding after quantizing. It worked significantly better, and blurring the image made the skeleton the best option we have found. After going through all of our images and finding the best blur and size, we talked to Alex, and he wants us to present our findings at the group meeting tomorrow. We made a big powerpoint that kind of sums up all the stuff we have tried that I will attach. We're going to read in a few new images (we have nearly 200 now, but we're still using the original 14) and see how well our code works with those. We're going to try to automate the blurring and size choice and then try to quantify the data. We also looked at histogram equalization, which seems to have a similar effect to the quantizing function. We're also looking into photoshop/image filters that could increase contrast. Also, we want to try to fit the image histograms to a gaussian to normalize the data, hoping that the blur and size will be almost the same. I'd say it's been a pretty productive month overall, and we're going to try to get as much done as we can next week before school starts.

Kate